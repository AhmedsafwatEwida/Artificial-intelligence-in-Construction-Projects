{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport urllib.request\nfrom bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, RidgeCV\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import functools\nfrom IPython.core.display import display, HTML\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', 10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data reading and Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"piping_df=pd.read_csv(\"/kaggle/input/pipingdata2/piping_df2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"piping_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"piping_df[piping_df['Country1']=='UAE'].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"piping_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot(piping_df,hue = 'Country1',diag_kind = 'kde',             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},             size = 4)\nsns.pairplot(piping_df,hue = 'Country1',diag_kind = 'kde',\n             vars = ['Erection','Daily_Rate'],\n             plot_kws = {'alpha': 0.6, 's': 120, 'edgecolor': 'k'},\n             size = 6)\n#vars = ['Pipe Fitter', 'Grinder', 'Riggers','Cranes','Pipe Welder (Argon)','Pipe Welder (CS)']\n#sns.legend(size = 14)\n# Title \n#plt.suptitle('Pair Plot of Skilled Labours',             size = 28);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot(piping_df,hue = 'Country1',diag_kind = 'kde',             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},             size = 4)\nsns.pairplot(piping_df,hue = 'Country1',diag_kind = 'kde',\n             vars = ['Pipe Welder (Argon)','Pipe Welder (CS)','Pipe Fitter'],\n             plot_kws = {'alpha': 0.6, 's': 120, 'edgecolor': 'k'},\n             size = 6)\n#vars = ['Pipe Fitter', 'Grinder', 'Riggers','Cranes','Pipe Welder (Argon)','Pipe Welder (CS)']\n#sns.legend(size = 14)\n# Title \n#plt.suptitle('Pair Plot of Skilled Labours',             size = 28);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import calendar\nimport datetime\nimport seaborn as sns\n#box, violin, strip\nselected_data=grouped_countries=piping_df.groupby(['Country1','Month'], as_index=False)[\"Erection\"].mean()\n\nHUE=piping_df['Country1']\nY=piping_df['Erection']\npiping_df['Month']=piping_df['Month'].astype(int)\npiping_df['Month'] = piping_df['Month'].apply(lambda x: calendar.month_abbr[x])\nX=piping_df['Month']\n\n\n\n#HUE=Combined_all_data[(Combined_all_data['offer_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Month', y='Erection' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Piping erection average per Month\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#box, violin, strip\nselected_data=grouped_countries=piping_df.groupby(['Country1','Month'], as_index=False)[\"Erection\"].mean()\n\nHUE=piping_df['Country1']\nY=piping_df['Daily_Rate']\nX=piping_df['Month']\n\n#HUE=Combined_all_data[(Combined_all_data['offer_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Month', y='Daily_Rate' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Piping average Daily Production rate per Month\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#box, violin, strip\nselected_data=grouped_countries=piping_df.groupby(['Country1','Year'], as_index=False)[\"Erection\"].mean()\n\nHUE=piping_df['Country1']\nY=piping_df['Erection']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offer_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Erection' , data=selected_data, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Piping erection average per Year\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#box, violin, strip\nselected_data=piping_df[(piping_df['Temperature/Heat index']=='Low') & (piping_df['Political issues']=='NO')& (piping_df['availability of Material']=='YES') & (piping_df['Holiday']=='No') & (piping_df['Fabricated Spools availability']=='High')& (piping_df['Distance between the spools fabrication Workshop and site(L.M)']=='Low') & (piping_df['Crew Experience']=='High') & (piping_df['crew supervision']=='High') & (piping_df['Work Front availability']=='High')& (piping_df['Drawings availability']=='High') & (piping_df['Working at heights']=='NO')]\n\nHUE=piping_df['Country1']\nY=piping_df['Daily_Rate']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offe]r_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Daily_Rate' , data=selected_data, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Piping average Daily Production rate per Year Execluding unusual Circumstances\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#box, violin, strip\nselected_data=piping_df[(piping_df['Temperature/Heat index']=='Low') & (piping_df['Political issues']=='NO')& (piping_df['availability of Material']=='YES') & (piping_df['Holiday']=='No') & (piping_df['Fabricated Spools availability']=='High')& (piping_df['Distance between the spools fabrication Workshop and site(L.M)']=='Low') & (piping_df['Crew Experience']=='High') & (piping_df['crew supervision']=='High') & (piping_df['Work Front availability']=='High')& (piping_df['Drawings availability']=='High') & (piping_df['Working at heights']=='NO')]\n\nHUE=piping_df['Country1']\nY=piping_df['Daily_Rate']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offe]r_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Daily_Rate' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Piping average Daily Production rate per Year\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#box, violin, strip\nselected_data=piping_df\n\nHUE=piping_df['Country1']\nY=piping_df['Riggers']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offe]r_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Riggers' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Average number of Riggers per Year\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#box, violin, strip\nselected_data=piping_df\n\nHUE=piping_df['Country1']\nY=piping_df['Grinder']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offe]r_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Grinder' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Average number of Grinders per Year\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#box, violin, strip\nselected_data=piping_df[(piping_df['Country1'] != 'Egypt')]\n\nHUE=piping_df['Country1']\nY=piping_df['Pipe Fitter']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offe]r_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Pipe Fitter' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Average number of Pipe Fitters per Year\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#box, violin, strip\nselected_data=piping_df[(piping_df['Country1'] != 'Egypt')]\n\nHUE=piping_df['Country1']\nY=piping_df['Pipe Welder (CS)']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offe]r_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Pipe Welder (CS)' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Average number of Pipe Welders(CS) per Year\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#box, violin, strip\nselected_data=piping_df[(piping_df['Country1'] != 'Egypt')]\n\nHUE=piping_df['Country1']\nY=piping_df['Pipe Welder (Argon)']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offe]r_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Pipe Welder (Argon)' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Average number of Pipe Welders (Argon) per Year\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#box, violin, strip\nselected_data=piping_df[(piping_df['Country1'] != 'Egypt')]\n\nHUE=piping_df['Country1']\nY=piping_df['Cranes']\nX=piping_df['Year']\n\n#HUE=Combined_all_data[(Combined_all_data['offe]r_id'] != '0')&( Combined_all_data['event'] == 'offer completed')]['offer_id']\n#Y=Combined_all_data[(Combined_all_data['offer_id'] != '0' )&( Combined_all_data['event'] == 'offer completed') ]['age']\n#X=Combined_all_data[(Combined_all_data['offer_id'] != '0') & (Combined_all_data['event'] == 'offer completed')]['gender']\nsns.factorplot(x='Year', y='Cranes' , data=piping_df, hue='Country1',size=12,kind='bar', aspect=1)\nplt.savefig(\"Average number of  Cranes per Year\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation for Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"modeled_data=piping_df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## converting categorial data to dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies_list=['Country1', 'HSE restrictions','Temperature/Heat index', 'Political issues', 'Material of Pipes ','Month','Year','Pipes size in D.I',\n       'availability of Material', 'Holiday','Fabricated Spools availability','Distance between the spools fabrication Workshop and site(L.M)',\n       'Crews Nationality', 'Crew Experience', 'crew supervision','Drawings availability', 'Work Front availability','Working at heights']\n\n\nfor col in dummies_list:\n    dummy = pd.get_dummies(modeled_data[col]).rename(columns=lambda x: col+'_' +str(x))\n    modeled_data = pd.concat([modeled_data, dummy], axis = 1)\nmodeled_data.drop(dummies_list, axis=1, inplace = True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeled_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying minmax Scaler to numerical Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmodeled_data[['Pipe Fitter', 'Grinder', 'Pipe Welder (CS)', 'Pipe Welder (Argon)','Daily_Rate',\n       'Riggers', 'Cranes']]= mms.fit_transform(modeled_data[['Pipe Fitter', 'Grinder', 'Pipe Welder (CS)', 'Pipe Welder (Argon)','Daily_Rate',\n       'Riggers', 'Cranes']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sb\nC_mat = modeled_data.corr()\nfig = plt.figure(figsize = (45,45))\nsb.heatmap(C_mat, vmax = 0.4,annot=True, square = True,annot_kws={'size':14},cbar=True,linewidths=4)\nsb.set(font_scale=3)\nplt.show()\nplt.savefig(\"Heat Map\"+\".png\", bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_mat = modeled_data.corr()\n#Correlation with output variable\ncor_target = C_mat[\"Erection\"]\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.01].sort_values(ascending=False)\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_mat = modeled_data.corr()\n#Correlation with output variable\ncor_target = C_mat[\"Erection\"]\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target<(-0.01)].sort_values()\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"modeled_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selecting Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Features=['Pipe Fitter', 'Grinder',\n       'Pipe Welder (CS)', 'Pipe Welder (Argon)', 'Riggers', 'Cranes',\n         'Country1_Egypt','Month_1', 'Month_2', 'Month_3', 'Month_4','Year_2005', 'Year_2006', 'Year_2007',\n       'Year_2008', 'Year_2009', 'Year_2010', 'Year_2011', 'Year_2012',\n       'Year_2013', 'Year_2014', 'Year_2015', 'Year_2016', 'Year_2017',\n       'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10',\n       'Month_11', 'Month_12','Country1_Oman', 'Country1_Qatar', 'Country1_Saudi', 'Country1_UAE',\n       'HSE restrictions_NO', 'HSE restrictions_YES',\n       'Temperature/Heat index_High', 'Temperature/Heat index_Low',\n       'Political issues_NO', 'Political issues_YES', 'Material of Pipes _CS',\n       'Material of Pipes _Duplex', 'Material of Pipes _LT',\n       'Material of Pipes _SS', 'availability of Material_NO',\n       'availability of Material_YES', 'Holiday_No', 'Holiday_Yes',\n       'Fabricated Spools availability_High', 'Pipes size in D.I_High', 'Pipes size in D.I_Low',\n       'Pipes size in D.I_Med',\n       'Fabricated Spools availability_Low',\n       'Distance between the spools fabrication Workshop and site(L.M)_High',\n       'Distance between the spools fabrication Workshop and site(L.M)_Low',\n       'Crews Nationality_Arab', 'Crews Nationality_others',\n       'Crew Experience_High', 'Crew Experience_Low', 'crew supervision_High',\n       'crew supervision_Low', 'Drawings availability_High',\n       'Drawings availability_Low', 'Work Front availability_High',\n       'Work Front availability_Low', 'Working at heights_NO',\n       'Working at heights_YES']'''\nFeatures=['Pipe Fitter', 'Grinder',\n       'Pipe Welder (CS)', 'Pipe Welder (Argon)', 'Riggers', 'Cranes',\n         'Country1_Egypt','Country1_Oman', 'Country1_Qatar', 'Country1_Saudi', 'Country1_UAE',\n       'HSE restrictions_NO', 'HSE restrictions_YES',\n       'Temperature/Heat index_High', 'Temperature/Heat index_Low',\n       'Political issues_NO', 'Political issues_YES', 'Material of Pipes _CS',\n       'Material of Pipes _Duplex', 'Material of Pipes _LT',\n       'Material of Pipes _SS', 'availability of Material_NO',\n       'availability of Material_YES', 'Holiday_No', 'Holiday_Yes',\n       'Fabricated Spools availability_High', 'Pipes size in D.I_High', 'Pipes size in D.I_Low',\n       'Pipes size in D.I_Med',\n       'Fabricated Spools availability_Low',\n       'Distance between the spools fabrication Workshop and site(L.M)_High',\n       'Distance between the spools fabrication Workshop and site(L.M)_Low',\n       'Crews Nationality_Arab', 'Crews Nationality_others',\n       'Crew Experience_High', 'Crew Experience_Low', 'crew supervision_High',\n       'crew supervision_Low', 'Drawings availability_High',\n       'Drawings availability_Low', 'Work Front availability_High',\n       'Work Front availability_Low', 'Working at heights_NO',\n       'Working at heights_YES']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\ndef prepare_Modeled_data(data):\n    \"\"\"Shuffling data.\"\"\"\n    data_train = data[Features]\n    labels_train = data['Erection']\n \n    \n    #Shuffle reviews and corresponding labels within training and test sets\n    data_train, labels_train = shuffle(data_train, labels_train)\n    \n    return data_train, labels_train\n\nfeatures,labels=prepare_Modeled_data(modeled_data)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeled_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_dev, y_train, y_dev = train_test_split(features,labels, test_size=0.10, random_state=42)\n\nprint(X_dev.shape)\nprint(y_dev.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import PCA\nk =5\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X_train)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca_result = pca.fit_transform(X_train)\n            \nprint(pca.components_)\nprint(pca.explained_variance_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\naxis_font = {'fontname':'Arial', 'size':'14'}\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"muted\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y_pred,sizes =100, legend='full',palette=palette)\nplt.title(\"PCA - Clustered (K-Means)\",**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Kmeans\"+\".png\", bbox_inches='tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=pca_result[:,0], \n    ys=pca_result[:,1], \n    zs=pca_result[:,2], \n    c=y_pred, \n    cmap='tab10'\n)\nax.set_xlabel('pca-1')\nax.set_ylabel('pca-2')\nax.set_zlabel('pca-3')\n\nplt.title(\"PCA - Clustered (K-Means)-3D\",**title_font,bbox={'facecolor':'0.9', 'pad':2})\nplt.savefig(\"Kmeans_3D\"+\".png\", bbox_inches=\"tight\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keras regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras import regularizers\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\ndef base_model():\n    model = Sequential()\n    model.add(Dense(200, input_dim=X_train.shape[1],kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal' ,activation='relu'))\n    model.add(Dense(100,kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    model.compile(loss='mean_squared_error', optimizer='adamax')\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KerasRegressor(build_fn=base_model, verbose=0)\nbatch_size = [50]\nepochs = [20,30,40]\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid,scoring='neg_mean_squared_error',n_jobs=2)\ngrid_result = grid.fit(X_train.values, y_train.values)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def base_model(l1=200,l2=100):\n    model = Sequential()\n    model.add(Dense(l1, input_dim=X_train.shape[1],kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal' ,activation='relu'))\n    model.add(Dense(l2,kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n    model.compile(loss='mean_squared_error', optimizer='Adamax')\n    return model\n\nmodel = KerasRegressor(build_fn=base_model, verbose=0, epochs=50, batch_size=50)\nl1 = [200,300]\nl2 = [50,100]\nparam_grid = dict(l1=l1, l2=l2)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid,scoring='neg_mean_squared_error')\ngrid_result = grid.fit(X_train.values, y_train.values)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nopt = Adam(lr=1e-3, decay=1e-3 / 200)\ndef base_model():\n    alpha=.00001\n    model = Sequential()\n    model.add(Dense(300, input_dim=X_train.shape[1],kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal' ,activation='relu'))\n    model.add(Dense(50,kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal',activation='relu',))\n    model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n    model.compile(loss='mean_squared_error', optimizer= opt)\n    return model\n\nreg = KerasRegressor(build_fn=base_model, epochs=50, batch_size=100,verbose=1,validation_split=0.1)\nkfold = KFold(n_splits=5, random_state=43)\nresults = np.sqrt(-1*cross_val_score(reg, X_train.values, y_train.values,scoring= \"neg_mean_squared_error\", cv=kfold))\nprint(\"Training RMSE mean and std from CV: {} {}\".format(results.mean(),results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\nhistory=reg.fit(X_train.values, y_train.values,epochs=50, batch_size=100, verbose=1, validation_split=0.1,callbacks=[es])\nprediction=reg.predict(X_dev.values)\nresult = np.sqrt(mean_squared_error(y_dev,prediction))\nprint(\"KerasRegressor_RMSE: {}\".format(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install PyYAML","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import model_from_json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.model.save('saved_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\n\n# Instantiate the model as you please (we are not going to use this)\nmodel2 = KerasRegressor(build_fn=base_model, epochs=10, batch_size=10, verbose=1)\n\n# This is where you load the actual saved model into new variable.\nmodel2.model = load_model('/kaggle/input/kerasmodel/saved_model.h5')\n\n# Now you can use this to predict on new data (without fitting model2, because it uses the older saved model)\nmodel2.predict(X_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#MinMaxScaler().fit_transform(X_new[0, :].reshape(1, -1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import array\nPipe_Fitter=300\nGrinder=600\nPipe_Welder_CS=400\nPipe_Welder_Argon=0\nRiggers=500\nCranes=350\nCountry1_Egypt=0\nCountry1_Oman=1\nCountry1_Qatar=0\nCountry1_Saudi=0\nCountry1_UAE=0\nHSE_restrictions_NO=1\nHSE_restrictions_YES=0\nTemperature_Heat_index_High=0\nTemperature_Heat_index_Low=1\nPolitical_issues_NO=1\nPolitical_issues_YES=0\nMaterial_of_Pipes_CS=1\nMaterial_of_Pipes_Duplex=0\nMaterial_of_Pipes_LT=0\nMaterial_of_Pipes_SS=0\navailability_of_Material_NO=0\navailability_of_Material_YES=1\nHoliday_No=1\nHoliday_Yes=0\nFabricated_Spools_availability_High=1\nPipessize_DI_High=0\nPipessize_DI_Low=0\nPipessize_DI_Med=1\nFabricated_Spools_availability_Low=0\nDistance=0\nDistance=1\nNationality_Arab=1\nNationality_others=0\nExperience_High=1\nExperience_Low=0\nsupervision_High=0\nsupervision_Low=0\nDrawings_availability_High=1\nDrawings_availability_Low=0\nWork_Front_availability_High=1\nWork_Front_availability_Low=0\nWorking_at_heights_NO=1\nWorking_at_heights_YES=0\nX_new=array([[Pipe_Fitter,\nGrinder,\nPipe_Welder_CS,\nPipe_Welder_Argon,\nRiggers,\nCranes,\nCountry1_Egypt,\nCountry1_Oman,\nCountry1_Qatar,\nCountry1_Saudi,\nCountry1_UAE,\nHSE_restrictions_NO,\nHSE_restrictions_YES,\nTemperature_Heat_index_High,\nTemperature_Heat_index_Low,\nPolitical_issues_NO,\nPolitical_issues_YES,\nMaterial_of_Pipes_CS,\nMaterial_of_Pipes_Duplex,\nMaterial_of_Pipes_LT,\nMaterial_of_Pipes_SS,\navailability_of_Material_NO,\navailability_of_Material_YES,\nHoliday_No,\nHoliday_Yes,\nFabricated_Spools_availability_High,\nPipessize_DI_High,\nPipessize_DI_Low,\nPipessize_DI_Med,\nFabricated_Spools_availability_Low,\nDistance,\nDistance,\nNationality_Arab,\nNationality_others,\nExperience_High,\nExperience_Low,\nsupervision_High,\nsupervision_Low,\nDrawings_availability_High,\nDrawings_availability_Low,\nWork_Front_availability_High,\nWork_Front_availability_Low,\nWorking_at_heights_NO,\nWorking_at_heights_YES]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_new=reg.predict(X_new)\nprediction_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''diff = prediction- y_dev\npercentDiff = (diff / y_dev) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.vis_utils import plot_model\nmodels = Sequential()\nmodels.add(Dense(300, input_dim=X_train.shape[1],kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal' ,activation='relu'))\nmodels.add(Dense(50,kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal',activation='relu',))\nmodels.add(Dense(1, kernel_initializer='normal',activation='linear'))\nmodels.compile(loss='mean_squared_error', optimizer= opt)\nplot_model(models, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot network\naxis_font = {'fontname':'Arial', 'size':'40'}\nfig, (ax1) = plt.subplots(1, 1, figsize=(60,20))\nplt.plot(history.history['loss'], label='Adam Regularized')\nplt.xlabel('Epochs',**axis_font)\nplt.ylabel('loss',**axis_font)\nplt.xticks(fontsize=40)\nplt.yticks(fontsize=40)\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nresults = permutation_importance(reg, X_train, y_train, scoring='neg_mean_squared_error')\n# get importance\nimportance = results.importances_mean\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\n#pyplot.bar([x for x in range(len(importance))], importance)\n#pyplot.show()\n\nimportant_features_list = sorted(importance,reverse=True)\nfig, (ax1) = plt.subplots(1, 1, figsize=(60,20))\nplt.yscale('log', nonposy='clip')\nplt.bar(range(len(important_features_list)), important_features_list, align='center')\nplt.xticks(range(len(important_features_list)), features, rotation='vertical',fontsize=40)\nplt.yticks(fontsize=40)\nplt.title('Keras Regressor-Features importance')\nplt.ylabel('Importance',fontsize=40)\nplt.xlabel('Features',fontsize=40)\nplt.savefig(\"Keras Regressor-Features importance\"+\".png\", bbox_inches='tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linera regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 5000)\npd.set_option('display.width', 10000)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\nregression=LinearRegression()\nregression.fit(X_train.values, y_train.values)\ny_pred_linear_regression=regression.predict(X_dev.values)\ny_Actual=y_dev\nLinear_RMSE=sqrt(mean_squared_error(y_Actual,y_pred_linear_regression))\nprint('Linear Regression_RMSE:',Linear_RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nresults = permutation_importance(regression, X_train, y_train, scoring='neg_mean_squared_error')\n# get importance\nimportance = results.importances_mean\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\n#pyplot.bar([x for x in range(len(importance))], importance)\n#pyplot.show()\n\nimportant_features_list = sorted(importance,reverse=True)\nfig, (ax1) = plt.subplots(1, 1, figsize=(60,20))\nplt.yscale('log', nonposy='clip')\nplt.bar(range(len(important_features_list)), important_features_list, align='center')\nplt.xticks(range(len(important_features_list)), features, rotation='vertical',fontsize=40)\nplt.yticks(fontsize=40)\nplt.title('Linear Regression-Features importance')\nplt.ylabel('Importance',fontsize=40)\nplt.xlabel('Features',fontsize=40)\nplt.savefig(\"Linear Regression-Features importance\"+\".png\", bbox_inches='tight')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''diff = y_pred_linear_regression - y_Actual\npercentDiff = (diff / y_Actual) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('Linear Regression: R^2 score on training set', regression.score(X_train, y_train)*100)\n#print('Linear Regression: R^2 score on test set', regression.score(X_dev, y_dev)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Elastic Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"elastic=ElasticNet(normalize=True)\nsearch=GridSearchCV(estimator=elastic,param_grid={'alpha':np.logspace(-5,2,8),'l1_ratio':[.2,.4,.6,.8]},scoring='neg_mean_squared_error',n_jobs=1,refit=True,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search.fit(X_train.values, y_train.values)\nsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elastic=ElasticNet(normalize=True,alpha=1e-05,l1_ratio=0.8)\nelastic.fit(X_train.values, y_train.values)\ny_pred_linear_elastic=elastic.predict(X_dev.values)\ny_Actual=y_dev\nelastic_model_RMSE=sqrt(mean_squared_error(y_Actual,y_pred_linear_elastic))\nprint('Elastic Regression_RMSE:',elastic_model_RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('Linear Regression: R^2 score on training set', elastic.score(X_train, y_train)*100)\n#print('Linear Regression: R^2 score on test set', elastic.score(X_dev, y_dev)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = y_pred_linear_elastic - y_Actual\npercentDiff = (diff / y_Actual) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridgereg = Ridge(alpha=0.0001,normalize=True)\nridgereg.fit(X_train.values, y_train.values)\ny_pred_ridge = ridgereg.predict(X_dev.values)\ny_Actual=y_dev\nRidge_model_RMSE=sqrt(mean_squared_error(y_Actual,y_pred_ridge))\nprint('Ridge Regression_RMSE:',Ridge_model_RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_ridge=GridSearchCV(estimator=ridgereg,param_grid={'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]},scoring='neg_mean_squared_error',n_jobs=1,refit=True,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_ridge.fit(X_train.values, y_train.values)\nsearch_ridge.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('Ridge Regression: R^2 score on training set', ridgereg.score(X_train, y_train)*100)\n#print('Ridge Regression: R^2 score on test set', ridgereg.score(X_dev, y_dev)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''diff = y_pred_ridge - y_Actual\npercentDiff = (diff / y_Actual) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlassoreg = Lasso(alpha=1)\nlassoreg.fit(X_train.values, y_train.values)\ny_pred_lasso = lassoreg.predict(X_dev.values)\ny_Actual=y_dev\nlassoreg_model_RMSE=sqrt(mean_squared_error(y_Actual,y_pred_lasso))\nprint('Lasso Regression_RMSE:',lassoreg_model_RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('Lasso Regression: R^2 score on training set', lassoreg.score(X_train, y_train)*100)\n#print('Lasso Regression: R^2 score on test set', lassoreg.score(X_dev, y_dev)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_lasso=GridSearchCV(estimator=lassoreg,param_grid={'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]},scoring='neg_mean_squared_error',n_jobs=1,refit=True,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_lasso.fit(X_train.values, y_train.values)\nsearch_lasso.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''diff = y_pred_lasso - y_Actual\npercentDiff = (diff / y_Actual) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(diff, bins = 25)\nplt.xlabel(\"Prediction Error [MPG]\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Random Forrest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random forest model specification\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix\ndef binary(movement):\n    \"\"\"\n    Converts percent change to a binary 1 or 0, where 1 is an increase and 0 is a decrease/no change\n    \n    \"\"\"\n    #Empty arrays where a 1 represents an increase in price and a 0 represents a decrease in price\n    direction = np.empty(movement.shape[0])\n    #If the change in price is greater than zero, store it as a 1\n    #If the change in price is less than zero, store it as a 0\n    for i in range(movement.shape[0]):\n        if movement[i] > 0:\n            direction[i] = 1\n        else:\n            direction[i]= 0\n    return direction\n\n\nregr = RandomForestRegressor(n_estimators=20, criterion='mse', max_depth=None, \n                      min_samples_split=2, min_samples_leaf=1, \n                      min_weight_fraction_leaf=0.0, max_features='auto', \n                      max_leaf_nodes=None, min_impurity_decrease=0.0, \n                      min_impurity_split=None, bootstrap=True, \n                      oob_score=False, n_jobs=1, random_state=None, \n                      verbose=2, warm_start=False)\n\n#Train on data\nregr.fit(X_train, y_train.ravel())\n\ny_pred_random = regr.predict(X_dev)\ny_dev = y_dev.to_frame()    \n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = regr, X = X_train, y = y_train, cv = 5)#cv is the number of test sets\nprint(accuracies.mean())\nprint(accuracies.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = regr.feature_importances_\nimportant_features_list = sorted(feature_importance,reverse=True)\nfig, (ax1) = plt.subplots(1, 1, figsize=(60,20))\n#plt.figure(figsize=(16, 6))\nplt.yscale('log', nonposy='clip')\nplt.bar(range(len(feature_importance)), important_features_list, align='center')\nplt.xticks(range(len(feature_importance)), features, rotation='vertical',fontsize=18)\nplt.yticks(fontsize=18)\nplt.title('RandomForestRegressor-Feature importance')\nplt.ylabel('Importance')\nplt.xlabel('Features')\nplt.savefig(\"RandomForestRegressor-Feature importance\"+\".png\", bbox_inches='tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''accuracies = cross_val_score(estimator = regr, X = X_dev, y = y_dev, cv = 5)#cv is the number of test sets\nprint(accuracies.mean())\nprint(accuracies.std())'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''diff = y_pred_random - y_dev\npercentDiff = (diff / y_dev) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#regr.score(X_dev,y_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_random = regr.predict(X_dev)\nRandomForrest_rmse = sqrt(mean_squared_error(y_dev, y_pred_random))\nprint('RandomForestRegressor_RMSE:',RandomForrest_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(diff, bins = 25)\nplt.xlabel(\"Prediction Error [MPG]\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\n\ngbm = xgb.XGBRegressor(objective=\"reg:linear\", seed=1729)\nreg_cv = GridSearchCV(gbm, {\"colsample_bytree\":[1.0],\"min_child_weight\":[1.2]\n                            ,'max_depth': [5], 'n_estimators': [100]}, verbose=1)\nreg_cv.fit(X_train,y_train,eval_metric='rmse', verbose = True, eval_set = [(X_dev, y_dev)])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#{'colsample_bytree': 1.0, 'max_depth': 5, 'min_child_weight': 1.2, 'n_estimators': 100}\nreg_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = xgb.XGBRegressor(**reg_cv.best_params_)\ngbm.fit(X_train,y_train)\ny_pred_xgboost = gbm.predict(X_dev)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBRegressor_RMSE = sqrt(mean_squared_error(y_dev, y_pred_xgboost))\nprint('XGBRegressor_RMSE:',XGBRegressor_RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''diff = y_pred_xgboost- y_dev\npercentDiff = (diff / y_dev) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std''''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = gbm, X = X_train, y = y_train, cv = 5)#cv is the number of test sets\nprint(accuracies.mean())\nprint(accuracies.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gbm.score(X_dev,y_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gbm.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# returns the weight that sum up to one.\nimportances=gbm.feature_importances_\nimportant_features_list = sorted(importances,reverse=True)\nfig, (ax1) = plt.subplots(1, 1, figsize=(60,20))\n\nplt.yscale('log', nonposy='clip')\nplt.bar(range(len(important_features_list)), important_features_list, align='center')\nplt.xticks(range(len(important_features_list)), features, rotation='vertical')\nplt.title('XGBRegressor-Features importance')\nplt.ylabel('Importance')\nplt.xlabel('Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#eturns occurrences of the features in splits. If you divide these occurrences by their sum, you'll get Item 1. Except here, features with 0 importance will be excluded.\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot as plt\nax = plot_importance(gbm)\nax.figure.set_size_inches(10,8)\n\n#fig = plt.figure(figsize=(10, 10))\n#plot_importance(gbm)\n\n#\n#plot_importance(gbm).plot(kind='barh', figsize=(60,40))\n#plt.show(plot_importance(gbm))\n#plt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(diff, bins = 25)\nplt.xlabel(\"Prediction Error [MPG]\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Caboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n\n#model_cat = CatBoostRegressor(iterations=5000, learning_rate=0.05, depth=5)\n# Fit model\n#reg_cv.fit(X_train,y_train,eval_metric='rmse', verbose = True, eval_set = [(X_dev, y_dev)])\n\n\n# Get predictions\n#preds = model.predict(test_pool)\nmodel_cat = CatBoostRegressor()\nparameters = {'depth'         : [6,8,10],\n                  'learning_rate' : [0.01, 0.05, 0.1],\n                  'iterations'    : [30, 50, 100]\n                 }\ngrid = GridSearchCV(estimator=model_cat, param_grid = parameters, cv = 2, n_jobs=-1)\ngrid.fit(X_train, y_train)    \n\n    # Results from Grid Search\nprint(\"\\n========================================================\")\nprint(\" Results from Grid Search \" )\nprint(\"========================================================\")    \n    \nprint(\"\\n The best estimator across ALL searched params:\\n\",\n          grid.best_estimator_)\n    \nprint(\"\\n The best score across ALL searched params:\\n\",\n          grid.best_score_)\n    \nprint(\"\\n The best parameters across ALL searched params:\\n\",\n          grid.best_params_)\n    \nprint(\"\\n ========================================================\")\n\n#y_pred_catboost = model_cat.predict(X_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cat = CatBoostRegressor(**grid.best_params_)\nmodel_cat.fit(X_train,y_train)\ny_pred_catboost = model_cat.predict(X_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CatBoostRegressor_RMSE = sqrt(mean_squared_error(y_dev, y_pred_catboost))\nprint('CatBoostRegressor_RMSE:',CatBoostRegressor_RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_cat.score(X_dev,y_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''diff = y_pred_catboost- y_dev\npercentDiff = (diff / y_dev) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(diff, bins = 25)\nplt.xlabel(\"Prediction Error [MPG]\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGBRegressor ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n'''lgb_model = lgb.LGBMRegressor(max_depth=6,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                               )'''\nlgb_model = lgb.LGBMRegressor()\nparameters = {'max_depth'         : [6,8,10],\n                  'learning_rate' : [0.01, 0.05, 0.1],\n                  'iterations'    : [30, 50, 100,500]\n                 }\ngrid = GridSearchCV(estimator=lgb_model, param_grid = parameters, cv = 2, n_jobs=-1)\ngrid.fit(X_train, y_train)    \n\n    # Results from Grid Search\nprint(\"\\n========================================================\")\nprint(\" Results from Grid Search \" )\nprint(\"========================================================\")    \n    \nprint(\"\\n The best estimator across ALL searched params:\\n\",\n          grid.best_estimator_)\n    \nprint(\"\\n The best score across ALL searched params:\\n\",\n          grid.best_score_)\n    \nprint(\"\\n The best parameters across ALL searched params:\\n\",\n          grid.best_params_)\n    \nprint(\"\\n ========================================================\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(**grid.best_params_)\nmodel_lgb.fit(X_train,y_train)\ny_pred_lgb = model_lgb.predict(X_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBMRegressor_RMSE = sqrt(mean_squared_error(y_dev, y_pred_lgb))\nprint('LGBMRegressor_RMSE:',LGBMRegressor_RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb.booster_.save_model('lgb_model.txt')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_performance = []\n\nModel_type = [\n                'Linear_regression',\n                   'Lasso_regression',\n                   'Ridge_regression',\n                   'Elastic_regression',\n                  'XGBRegressor',\n                  'LGBRegressor',\n                   'RandomForestRegressor',\n                    'KerasRegressor',\n                  'CatBoostRegressor']\nmodels_performance.append(Linear_RMSE)\nmodels_performance.append(lassoreg_model_RMSE)\nmodels_performance.append(Ridge_model_RMSE)\nmodels_performance.append(elastic_model_RMSE)\nmodels_performance.append(XGBRegressor_RMSE)\nmodels_performance.append(LGBMRegressor_RMSE)\nmodels_performance.append(RandomForrest_rmse)\nmodels_performance.append(result)\nmodels_performance.append(CatBoostRegressor_RMSE)\n\n\nmodels_performance= pd.DataFrame(models_performance,columns=['RMSE'])\n\nModel_type = pd.DataFrame(Model_type,\n                               columns=['Model_Type'])\n\nmodels_performance = pd.concat([Model_type, models_performance], axis=1)\n\nmodels_performance = models_performance.sort_values('RMSE', ascending=True)\nmodels_performance = models_performance.reset_index(drop=True)\nmodels_performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bst = lgb.Booster(model_file='/kaggle/input/lgbmodel/lgb_model.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_new=bst.predict(X_new)\nprediction_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_lgb.score(X_dev,y_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''diff = y_pred_lgb- y_dev\npercentDiff = (diff / y_dev) * 100\nabsPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\nmean,std\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(diff, bins = 25)\nplt.xlabel(\"Prediction Error [MPG]\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import vstack\nfrom numpy import sqrt\nfrom pandas import read_csv\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch import Tensor\nfrom torch.nn import Linear\nfrom torch.nn import Sigmoid\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import MSELoss\nfrom torch.nn.init import xavier_uniform_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset definition\nclass CSVDataset(Dataset):\n    # load the dataset\n    def __init__(self, data):\n        # load the csv file as a dataframe\n        df = data.copy()\n        # store the inputs and outputs\n        self.X = df.values[:, :-1].astype('float32')\n        self.y = df.values[:, -1].astype('float32')\n        # ensure target has the right shape\n        self.y = self.y.reshape((len(self.y), 1))\n \n    # number of rows in the dataset\n    def __len__(self):\n        return len(self.X)\n \n    # get a row at an index\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n \n    # get indexes for train and test rows\n    def get_splits(self, n_test=0.10):\n        # determine sizes\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # calculate the split\n        return random_split(self, [train_size, test_size])\n    def get_splits_test(self, n_test=1):\n           # determine sizes\n        test_size = round(n_test * len(self.X))\n       # train_size = len(self.X) - test_size\n        # calculate the split\n        return random_split(self, [test_size])\n \n# model definition\nclass MLP(Module):\n    # define model elements\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        # input to first hidden layer\n        self.hidden1 = Linear(n_inputs, 40)\n        xavier_uniform_(self.hidden1.weight)\n        self.act1 = Sigmoid()\n        # second hidden layer\n        self.hidden2 = Linear(40, 20)\n        xavier_uniform_(self.hidden2.weight)\n        self.act2 = Sigmoid()\n        \n        self.hidden3 = Linear(20, 10)\n        xavier_uniform_(self.hidden3.weight)\n        self.act3 = Sigmoid()\n        \n        \n        \n        # third hidden layer and output\n        self.hidden4 = Linear(10, 1)\n        xavier_uniform_(self.hidden4.weight)\n \n    # forward propagate input\n    def forward(self, X):\n        # input to first hidden layer\n        X = self.hidden1(X)\n        X = self.act1(X)\n         # second hidden layer\n        X = self.hidden2(X)\n        X = self.act2(X)\n        \n        X = self.hidden3(X)\n        X = self.act3(X)\n        \n        # third hidden layer and output\n        X = self.hidden4(X)\n        return X\n \n# prepare the dataset\ndef prepare_data(path):\n    # load the dataset\n    dataset = CSVDataset(path)\n    # calculate split\n    train, test = dataset.get_splits()\n    # prepare data loaders\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl\ndef prepare_data_test(df_test):\n    # load the dataset\n    \n   \n    # prepare data loaders\n    #train_dl2 = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl2 = DataLoader(df_test, batch_size=32, shuffle=False)\n    return  test_dl2 \n# train the model\ndef train_model(train_dl, model):\n    # define the optimization\n    criterion = MSELoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # enumerate epochs\n    for epoch in range(100):\n        # enumerate mini batches\n        for i, (inputs, targets) in enumerate(train_dl):\n            # clear the gradients\n            optimizer.zero_grad()\n            # compute the model output\n            yhat = model(inputs)\n            # calculate loss\n            loss = criterion(yhat, targets)\n            # credit assignment\n            loss.backward()\n            # update model weights\n            optimizer.step()\n \n# evaluate the model\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # evaluate the model on the test set\n        yhat = model(inputs)\n        # retrieve numpy array\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        actual = actual.reshape((len(actual), 1))\n        # store\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    diff = predictions - actuals\n    percentDiff = (diff / actuals) * 100\n    absPercentDiff = np.abs(percentDiff)\n# compute the mean and standard deviation of the absolute percentage\n# difference\n    mean = np.mean(absPercentDiff)\n    std = np.std(absPercentDiff)\n\n    # calculate mse\n    mse = sqrt(mean_squared_error(actuals, predictions))\n    \n    return mse,mean,std\ndef evaluate_model_test(df, model):\n    predictions, actuals = list(), list()\n    \n    for i, (inputs, targets) in enumerate(df):\n        # evaluate the model on the test set\n        yhat = model(inputs)\n        # retrieve numpy array\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        actual = actual.reshape((len(actual), 1))\n        # store\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # calculate mse\n    mse = mean_squared_error(actuals, predictions)\n    return mse ,predictions, actuals\n# make a class prediction for one row of data\ndef predict(row, model):\n    # convert row to data\n    row = Tensor([row])\n    # make prediction\n    yhat = model(row)\n    # retrieve numpy array\n    yhat = yhat.detach().numpy()\n    return yhat\n \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"modeled_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=modeled_data[Features]\ndf2=modeled_data['Erection']\n\nmodeled_data1 = pd.concat([df1,df2],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the data\ndata = modeled_data1.copy()\ntrain_dl, test_dl = prepare_data(data)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# define the network\nmodel = MLP(44)\n# train the model\ntrain_model(train_dl, model)\n# evaluate the model\nmse,mean,std = evaluate_model(test_dl, model)\nprint('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n# make a single prediction (expect class=1)\n#row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n#yhat = predict(row, model)\n#print('Predicted: %.3f' % yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse,mean,std = evaluate_model(test_dl, model)\nmse,mean,std","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}